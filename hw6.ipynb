{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('DS_hw6_p1.csv')\n",
    "\n",
    "# preprocessing\n",
    "df_x = preprocessing.scale(df.iloc[:,:-2], axis = 0)\n",
    "train_x = df_x[:280, 1:]\n",
    "test_x = df_x[280:, 1:]\n",
    "train_y = df.iloc[:280, -2:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.814,  0.002,  1.512,  0.384],\n",
       "       [-1.113, -0.5  , -0.135,  1.401],\n",
       "       [ 0.492, -0.705,  1.832,  0.743],\n",
       "       ...,\n",
       "       [-0.571, -0.254,  0.175, -1.311],\n",
       "       [ 0.938, -0.316,  1.022, -0.444],\n",
       "       [ 0.322,  2.31 ,  0.26 , -1.002]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(280, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.825, -1.055],\n",
       "       [ 0.276, -0.415],\n",
       "       [ 0.78 , -0.502],\n",
       "       [ 0.32 , -0.352],\n",
       "       [-0.64 ,  0.505],\n",
       "       [ 0.709, -0.597],\n",
       "       [ 0.141,  0.344],\n",
       "       [ 0.836, -1.032],\n",
       "       [ 0.154, -0.096],\n",
       "       [-0.518,  0.481],\n",
       "       [ 0.304, -0.047],\n",
       "       [ 0.699, -0.5  ],\n",
       "       [ 0.308, -0.202],\n",
       "       [ 1.003, -1.052],\n",
       "       [ 0.116, -0.04 ],\n",
       "       [ 0.059,  0.056],\n",
       "       [-0.991,  1.238],\n",
       "       [ 0.102,  0.07 ],\n",
       "       [ 0.48 , -0.545],\n",
       "       [ 0.223, -0.336],\n",
       "       [ 0.647, -0.649],\n",
       "       [-0.64 ,  0.568],\n",
       "       [ 0.535, -0.317],\n",
       "       [-0.1  ,  0.102],\n",
       "       [-0.404,  0.822],\n",
       "       [ 0.178, -0.333],\n",
       "       [ 0.214, -0.22 ],\n",
       "       [ 0.678, -0.716],\n",
       "       [ 1.213, -1.399],\n",
       "       [-0.223, -0.012],\n",
       "       [ 0.245, -0.586],\n",
       "       [ 0.424, -0.479],\n",
       "       [ 0.537, -0.302],\n",
       "       [ 0.675, -0.663],\n",
       "       [-0.158, -0.145],\n",
       "       [ 0.466, -0.271],\n",
       "       [-0.397,  0.111],\n",
       "       [ 0.231, -0.64 ],\n",
       "       [-0.651,  0.789],\n",
       "       [ 0.951, -0.358],\n",
       "       [-0.946,  1.068],\n",
       "       [ 0.83 , -0.64 ],\n",
       "       [-0.08 , -0.245],\n",
       "       [ 0.166, -0.498],\n",
       "       [ 0.333, -0.25 ],\n",
       "       [-1.464,  1.549],\n",
       "       [-0.324,  0.323],\n",
       "       [ 0.559, -0.561],\n",
       "       [ 0.343, -0.436],\n",
       "       [-0.492,  0.57 ],\n",
       "       [-0.301,  0.225],\n",
       "       [-1.049,  0.758],\n",
       "       [-1.07 ,  1.054],\n",
       "       [ 0.223, -0.648],\n",
       "       [ 0.202, -0.376],\n",
       "       [-0.148, -0.089],\n",
       "       [ 0.559, -0.331],\n",
       "       [ 0.398, -0.482],\n",
       "       [ 0.471, -0.391],\n",
       "       [-1.306,  1.253],\n",
       "       [ 1.019, -0.78 ],\n",
       "       [ 1.016, -1.015],\n",
       "       [ 0.243,  0.088],\n",
       "       [ 0.309, -0.047],\n",
       "       [ 0.432, -0.341],\n",
       "       [ 0.59 , -0.355],\n",
       "       [ 0.391, -0.087],\n",
       "       [ 0.355, -0.05 ],\n",
       "       [-2.952,  2.89 ],\n",
       "       [ 0.256, -0.271],\n",
       "       [ 0.496, -0.824],\n",
       "       [-1.715,  1.94 ],\n",
       "       [ 1.04 , -1.145],\n",
       "       [-0.401,  0.436],\n",
       "       [ 1.051, -1.126],\n",
       "       [ 0.53 , -0.512],\n",
       "       [ 0.073,  0.082],\n",
       "       [-7.219,  7.434],\n",
       "       [ 0.459, -0.337],\n",
       "       [ 0.029,  0.295],\n",
       "       [-0.287,  0.153],\n",
       "       [ 0.936, -0.587],\n",
       "       [ 0.304, -0.218],\n",
       "       [-0.682,  0.777],\n",
       "       [ 1.459, -1.314],\n",
       "       [ 0.253, -0.398],\n",
       "       [ 0.11 , -0.131],\n",
       "       [ 0.505, -0.498],\n",
       "       [ 0.462, -0.222],\n",
       "       [ 0.09 , -0.245],\n",
       "       [ 1.324, -1.164],\n",
       "       [ 0.161, -0.09 ],\n",
       "       [ 0.11 ,  0.022],\n",
       "       [ 0.737, -0.627],\n",
       "       [ 0.984, -0.762],\n",
       "       [ 0.988, -0.975],\n",
       "       [ 0.824, -0.654],\n",
       "       [ 0.473, -0.163],\n",
       "       [ 0.514, -0.345],\n",
       "       [-0.202, -0.028],\n",
       "       [-0.431,  0.27 ],\n",
       "       [ 0.42 , -0.639],\n",
       "       [ 0.232, -0.682],\n",
       "       [ 0.683, -0.887],\n",
       "       [-0.524,  0.259],\n",
       "       [ 0.707, -0.381],\n",
       "       [-2.133,  2.17 ],\n",
       "       [-0.63 ,  0.448],\n",
       "       [ 0.304, -0.501],\n",
       "       [ 1.341, -1.524],\n",
       "       [-0.65 ,  0.54 ],\n",
       "       [ 0.156,  0.318],\n",
       "       [ 0.026, -0.104],\n",
       "       [-2.544,  2.485],\n",
       "       [ 0.475, -0.714],\n",
       "       [ 0.966, -1.086],\n",
       "       [-0.968,  0.531],\n",
       "       [ 0.173,  0.104],\n",
       "       [ 0.85 , -0.599],\n",
       "       [ 0.128, -0.135],\n",
       "       [ 0.221, -0.118],\n",
       "       [ 0.505, -0.823],\n",
       "       [-3.194,  3.364],\n",
       "       [ 0.175, -0.245],\n",
       "       [-0.308, -0.128],\n",
       "       [-0.062, -0.044],\n",
       "       [-0.036,  0.431],\n",
       "       [-0.167, -0.24 ],\n",
       "       [ 0.737, -0.73 ],\n",
       "       [-1.608,  1.133],\n",
       "       [-1.479,  0.967],\n",
       "       [ 0.329, -0.408],\n",
       "       [ 0.427, -0.455],\n",
       "       [-1.865,  1.732],\n",
       "       [ 0.165,  0.16 ],\n",
       "       [ 0.404, -0.67 ],\n",
       "       [ 0.124, -0.075],\n",
       "       [-0.331,  0.299],\n",
       "       [ 0.519, -0.872],\n",
       "       [ 0.794, -0.422],\n",
       "       [ 0.202, -0.72 ],\n",
       "       [ 0.52 , -0.443],\n",
       "       [ 0.738, -0.527],\n",
       "       [-1.114,  1.193],\n",
       "       [-0.327, -0.147],\n",
       "       [-2.108,  2.374],\n",
       "       [-1.724,  2.104],\n",
       "       [-0.066, -0.091],\n",
       "       [-0.568,  0.264],\n",
       "       [-0.072,  0.314],\n",
       "       [-1.359,  1.445],\n",
       "       [-4.707,  4.593],\n",
       "       [-0.769,  0.634],\n",
       "       [ 0.843, -0.658],\n",
       "       [ 0.627, -0.733],\n",
       "       [ 0.044, -0.132],\n",
       "       [ 0.822, -0.63 ],\n",
       "       [ 0.68 , -0.873],\n",
       "       [ 0.895, -0.491],\n",
       "       [-0.221,  0.427],\n",
       "       [ 0.239,  0.327],\n",
       "       [ 0.317, -0.389],\n",
       "       [ 0.505, -0.436],\n",
       "       [-0.962,  0.829],\n",
       "       [ 1.124, -1.155],\n",
       "       [ 0.117, -0.385],\n",
       "       [-0.178,  0.467],\n",
       "       [-1.185,  1.518],\n",
       "       [ 0.411, -0.847],\n",
       "       [ 0.727, -0.565],\n",
       "       [-0.453,  0.524],\n",
       "       [ 1.038, -0.771],\n",
       "       [ 0.69 , -0.537],\n",
       "       [ 0.37 , -0.355],\n",
       "       [-0.653,  0.628],\n",
       "       [ 0.589, -1.055],\n",
       "       [ 0.452, -0.56 ],\n",
       "       [-0.445,  0.436],\n",
       "       [-1.084,  0.64 ],\n",
       "       [ 0.552, -0.667],\n",
       "       [ 0.051, -0.058],\n",
       "       [ 1.086, -0.659],\n",
       "       [ 0.595, -0.571],\n",
       "       [ 0.912, -1.166],\n",
       "       [-1.622,  1.379],\n",
       "       [-0.857,  0.854],\n",
       "       [ 0.667, -0.825],\n",
       "       [ 0.863, -0.38 ],\n",
       "       [-0.19 ,  0.187],\n",
       "       [ 0.917, -0.601],\n",
       "       [ 0.047, -0.264],\n",
       "       [-0.281,  0.147],\n",
       "       [ 0.783, -1.007],\n",
       "       [-0.144,  0.162],\n",
       "       [ 0.143, -0.379],\n",
       "       [ 0.457, -0.301],\n",
       "       [ 0.335, -0.402],\n",
       "       [ 0.572, -0.28 ],\n",
       "       [ 0.593, -0.558],\n",
       "       [-0.302, -0.35 ],\n",
       "       [ 0.283, -0.524],\n",
       "       [ 0.341, -0.551],\n",
       "       [-0.542,  0.319],\n",
       "       [-0.041,  0.065],\n",
       "       [ 0.017,  0.046],\n",
       "       [ 0.783, -0.702],\n",
       "       [ 0.882, -0.931],\n",
       "       [ 0.633, -0.842],\n",
       "       [ 0.578, -0.192],\n",
       "       [ 0.621, -0.454],\n",
       "       [ 0.246, -0.238],\n",
       "       [ 0.791, -0.714],\n",
       "       [ 0.245, -0.097],\n",
       "       [ 0.642, -0.283],\n",
       "       [-0.159,  0.175],\n",
       "       [-2.802,  3.02 ],\n",
       "       [-0.349,  0.345],\n",
       "       [-1.807,  1.677],\n",
       "       [-1.523,  1.467],\n",
       "       [-0.736,  0.717],\n",
       "       [ 0.353, -0.474],\n",
       "       [ 0.095,  0.073],\n",
       "       [ 0.439, -0.514],\n",
       "       [-0.496,  0.485],\n",
       "       [ 0.652, -0.397],\n",
       "       [ 0.326, -0.01 ],\n",
       "       [ 0.929, -0.775],\n",
       "       [ 0.272, -0.321],\n",
       "       [-2.226,  2.458],\n",
       "       [ 0.19 , -0.216],\n",
       "       [ 1.348, -1.083],\n",
       "       [ 0.683, -0.453],\n",
       "       [-0.427,  0.581],\n",
       "       [ 0.323, -0.139],\n",
       "       [ 0.878, -0.732],\n",
       "       [-3.659,  3.447],\n",
       "       [ 0.856, -0.952],\n",
       "       [ 0.279,  0.082],\n",
       "       [ 0.551, -0.837],\n",
       "       [ 0.15 , -0.372],\n",
       "       [ 0.356, -0.415],\n",
       "       [ 0.274, -0.613],\n",
       "       [ 0.061,  0.181],\n",
       "       [-2.744,  2.472],\n",
       "       [-1.197,  1.603],\n",
       "       [ 0.476, -0.529],\n",
       "       [-0.948,  0.954],\n",
       "       [ 0.364, -0.35 ],\n",
       "       [-1.85 ,  2.099],\n",
       "       [ 0.357, -0.537],\n",
       "       [-1.571,  1.269],\n",
       "       [-0.535,  0.581],\n",
       "       [ 0.722, -1.025],\n",
       "       [ 0.711, -0.479],\n",
       "       [ 0.639, -0.877],\n",
       "       [ 0.542, -0.938],\n",
       "       [-1.482,  1.05 ],\n",
       "       [ 0.046,  0.112],\n",
       "       [-1.518,  1.016],\n",
       "       [ 0.064, -0.178],\n",
       "       [-0.217,  0.039],\n",
       "       [ 0.579, -0.799],\n",
       "       [-1.23 ,  0.909],\n",
       "       [ 0.605, -0.813],\n",
       "       [ 0.727, -0.957],\n",
       "       [-0.383,  0.76 ],\n",
       "       [ 0.108,  0.11 ],\n",
       "       [ 0.65 , -0.498],\n",
       "       [ 0.79 , -0.535],\n",
       "       [ 0.121, -0.631],\n",
       "       [ 0.817, -0.407],\n",
       "       [ 0.344, -0.097],\n",
       "       [-0.017, -0.064],\n",
       "       [ 0.626, -0.689],\n",
       "       [ 0.622, -0.451],\n",
       "       [ 0.047,  0.245],\n",
       "       [-0.04 ,  0.001],\n",
       "       [ 0.844, -1.047],\n",
       "       [ 0.999, -0.778],\n",
       "       [-3.022,  3.063]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(280, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.Sequential([\n",
    "    layers.experimental.preprocessing.Normalization(input_shape=[4,]),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization_14 (Normalizat (None, 4)                 9         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 512)               2560      \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 528,907\n",
      "Trainable params: 528,898\n",
      "Non-trainable params: 9\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.00003),\n",
    "    loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.9553 - val_loss: 1.0912\n",
      "Epoch 2/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9621 - val_loss: 1.0841\n",
      "Epoch 3/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9473 - val_loss: 1.0766\n",
      "Epoch 4/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9549 - val_loss: 1.0683\n",
      "Epoch 5/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9843 - val_loss: 1.0607\n",
      "Epoch 6/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9424 - val_loss: 1.0543\n",
      "Epoch 7/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9628 - val_loss: 1.0478\n",
      "Epoch 8/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8859 - val_loss: 1.0417\n",
      "Epoch 9/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9588 - val_loss: 1.0352\n",
      "Epoch 10/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8989 - val_loss: 1.0285\n",
      "Epoch 11/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9447 - val_loss: 1.0224\n",
      "Epoch 12/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9115 - val_loss: 1.0164\n",
      "Epoch 13/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9031 - val_loss: 1.0094\n",
      "Epoch 14/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9153 - val_loss: 1.0038\n",
      "Epoch 15/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8751 - val_loss: 0.9983\n",
      "Epoch 16/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8725 - val_loss: 0.9925\n",
      "Epoch 17/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9022 - val_loss: 0.9874\n",
      "Epoch 18/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8871 - val_loss: 0.9817\n",
      "Epoch 19/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9106 - val_loss: 0.9758\n",
      "Epoch 20/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8832 - val_loss: 0.9692\n",
      "Epoch 21/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8707 - val_loss: 0.9634\n",
      "Epoch 22/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8938 - val_loss: 0.9580\n",
      "Epoch 23/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8289 - val_loss: 0.9530\n",
      "Epoch 24/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8561 - val_loss: 0.9472\n",
      "Epoch 25/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8590 - val_loss: 0.9415\n",
      "Epoch 26/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8497 - val_loss: 0.9359\n",
      "Epoch 27/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8125 - val_loss: 0.9293\n",
      "Epoch 28/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8176 - val_loss: 0.9231\n",
      "Epoch 29/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8237 - val_loss: 0.9172\n",
      "Epoch 30/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8034 - val_loss: 0.9113\n",
      "Epoch 31/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8066 - val_loss: 0.9052\n",
      "Epoch 32/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8561 - val_loss: 0.8991\n",
      "Epoch 33/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8531 - val_loss: 0.8932\n",
      "Epoch 34/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.7991 - val_loss: 0.8870\n",
      "Epoch 35/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8090 - val_loss: 0.8814\n",
      "Epoch 36/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8145 - val_loss: 0.8761\n",
      "Epoch 37/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8221 - val_loss: 0.8712\n",
      "Epoch 38/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.7906 - val_loss: 0.8663\n",
      "Epoch 39/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8089 - val_loss: 0.8618\n",
      "Epoch 40/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.7917 - val_loss: 0.8572\n",
      "Epoch 41/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8338 - val_loss: 0.8517\n",
      "Epoch 42/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.7832 - val_loss: 0.8452\n",
      "Epoch 43/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.7862 - val_loss: 0.8390\n",
      "Epoch 44/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.7740 - val_loss: 0.8336\n",
      "Epoch 45/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.7994 - val_loss: 0.8270\n",
      "Epoch 46/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.7889 - val_loss: 0.8215\n",
      "Epoch 47/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.7780 - val_loss: 0.8152\n",
      "Epoch 48/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.7688 - val_loss: 0.8091\n",
      "Epoch 49/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.7842 - val_loss: 0.8031\n",
      "Epoch 50/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.7186 - val_loss: 0.7972\n",
      "Epoch 51/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.7251 - val_loss: 0.7897\n",
      "Epoch 52/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.7254 - val_loss: 0.7824\n",
      "Epoch 53/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.7135 - val_loss: 0.7749\n",
      "Epoch 54/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6943 - val_loss: 0.7683\n",
      "Epoch 55/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.7118 - val_loss: 0.7622\n",
      "Epoch 56/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.7475 - val_loss: 0.7562\n",
      "Epoch 57/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6881 - val_loss: 0.7489\n",
      "Epoch 58/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.7210 - val_loss: 0.7424\n",
      "Epoch 59/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.7106 - val_loss: 0.7352\n",
      "Epoch 60/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.7099 - val_loss: 0.7295\n",
      "Epoch 61/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6685 - val_loss: 0.7225\n",
      "Epoch 62/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6726 - val_loss: 0.7154\n",
      "Epoch 63/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.7387 - val_loss: 0.7087\n",
      "Epoch 64/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6862 - val_loss: 0.7028\n",
      "Epoch 65/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6901 - val_loss: 0.6962\n",
      "Epoch 66/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6991 - val_loss: 0.6890\n",
      "Epoch 67/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6744 - val_loss: 0.6819\n",
      "Epoch 68/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6734 - val_loss: 0.6741\n",
      "Epoch 69/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.7093 - val_loss: 0.6683\n",
      "Epoch 70/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.7055 - val_loss: 0.6613\n",
      "Epoch 71/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.7046 - val_loss: 0.6554\n",
      "Epoch 72/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6784 - val_loss: 0.6486\n",
      "Epoch 73/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6477 - val_loss: 0.6418\n",
      "Epoch 74/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6724 - val_loss: 0.6350\n",
      "Epoch 75/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6325 - val_loss: 0.6289\n",
      "Epoch 76/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.5920 - val_loss: 0.6214\n",
      "Epoch 77/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6278 - val_loss: 0.6141\n",
      "Epoch 78/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6274 - val_loss: 0.6067\n",
      "Epoch 79/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6641 - val_loss: 0.6008\n",
      "Epoch 80/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5618 - val_loss: 0.5929\n",
      "Epoch 81/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6095 - val_loss: 0.5855\n",
      "Epoch 82/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.5730 - val_loss: 0.5785\n",
      "Epoch 83/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.6516 - val_loss: 0.5718\n",
      "Epoch 84/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6098 - val_loss: 0.5645\n",
      "Epoch 85/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6152 - val_loss: 0.5581\n",
      "Epoch 86/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6109 - val_loss: 0.5505\n",
      "Epoch 87/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6240 - val_loss: 0.5440\n",
      "Epoch 88/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5764 - val_loss: 0.5378\n",
      "Epoch 89/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5956 - val_loss: 0.5307\n",
      "Epoch 90/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5475 - val_loss: 0.5251\n",
      "Epoch 91/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6416 - val_loss: 0.5194\n",
      "Epoch 92/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5240 - val_loss: 0.5125\n",
      "Epoch 93/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5693 - val_loss: 0.5053\n",
      "Epoch 94/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5539 - val_loss: 0.4974\n",
      "Epoch 95/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5212 - val_loss: 0.4903\n",
      "Epoch 96/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5745 - val_loss: 0.4839\n",
      "Epoch 97/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5970 - val_loss: 0.4768\n",
      "Epoch 98/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5278 - val_loss: 0.4703\n",
      "Epoch 99/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5192 - val_loss: 0.4648\n",
      "Epoch 100/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5723 - val_loss: 0.4567\n",
      "Epoch 101/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5535 - val_loss: 0.4511\n",
      "Epoch 102/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4951 - val_loss: 0.4447\n",
      "Epoch 103/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4810 - val_loss: 0.4376\n",
      "Epoch 104/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4720 - val_loss: 0.4309\n",
      "Epoch 105/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5189 - val_loss: 0.4231\n",
      "Epoch 106/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5124 - val_loss: 0.4169\n",
      "Epoch 107/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4827 - val_loss: 0.4109\n",
      "Epoch 108/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4816 - val_loss: 0.4050\n",
      "Epoch 109/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4994 - val_loss: 0.3990\n",
      "Epoch 110/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4960 - val_loss: 0.3922\n",
      "Epoch 111/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5070 - val_loss: 0.3855\n",
      "Epoch 112/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.4434 - val_loss: 0.3778\n",
      "Epoch 113/500\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.238 - 0s 6ms/step - loss: 0.4484 - val_loss: 0.3721\n",
      "Epoch 114/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5123 - val_loss: 0.3663\n",
      "Epoch 115/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4820 - val_loss: 0.3604\n",
      "Epoch 116/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4590 - val_loss: 0.3533\n",
      "Epoch 117/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4387 - val_loss: 0.3473\n",
      "Epoch 118/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4679 - val_loss: 0.3422\n",
      "Epoch 119/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4615 - val_loss: 0.3359\n",
      "Epoch 120/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4423 - val_loss: 0.3303\n",
      "Epoch 121/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4417 - val_loss: 0.3250\n",
      "Epoch 122/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4226 - val_loss: 0.3209\n",
      "Epoch 123/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3871 - val_loss: 0.3151\n",
      "Epoch 124/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4857 - val_loss: 0.3092\n",
      "Epoch 125/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4682 - val_loss: 0.3022\n",
      "Epoch 126/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4551 - val_loss: 0.2953\n",
      "Epoch 127/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4413 - val_loss: 0.2897\n",
      "Epoch 128/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4016 - val_loss: 0.2831\n",
      "Epoch 129/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3479 - val_loss: 0.2772\n",
      "Epoch 130/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4022 - val_loss: 0.2736\n",
      "Epoch 131/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4090 - val_loss: 0.2680\n",
      "Epoch 132/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3744 - val_loss: 0.2641\n",
      "Epoch 133/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3952 - val_loss: 0.2600\n",
      "Epoch 134/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3342 - val_loss: 0.2572\n",
      "Epoch 135/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3437 - val_loss: 0.2538\n",
      "Epoch 136/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4568 - val_loss: 0.2505\n",
      "Epoch 137/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4312 - val_loss: 0.2474\n",
      "Epoch 138/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4079 - val_loss: 0.2431\n",
      "Epoch 139/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4462 - val_loss: 0.2390\n",
      "Epoch 140/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3962 - val_loss: 0.2349\n",
      "Epoch 141/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4005 - val_loss: 0.2325\n",
      "Epoch 142/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3162 - val_loss: 0.2280\n",
      "Epoch 143/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3653 - val_loss: 0.2247\n",
      "Epoch 144/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3214 - val_loss: 0.2214\n",
      "Epoch 145/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3499 - val_loss: 0.2185\n",
      "Epoch 146/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3149 - val_loss: 0.2146\n",
      "Epoch 147/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4234 - val_loss: 0.2118\n",
      "Epoch 148/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3892 - val_loss: 0.2084\n",
      "Epoch 149/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3371 - val_loss: 0.2052\n",
      "Epoch 150/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3162 - val_loss: 0.2035\n",
      "Epoch 151/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3350 - val_loss: 0.2006\n",
      "Epoch 152/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2943 - val_loss: 0.1959\n",
      "Epoch 153/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3285 - val_loss: 0.1938\n",
      "Epoch 154/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3927 - val_loss: 0.1893\n",
      "Epoch 155/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2958 - val_loss: 0.1870\n",
      "Epoch 156/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3479 - val_loss: 0.1858\n",
      "Epoch 157/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2439 - val_loss: 0.1839\n",
      "Epoch 158/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3100 - val_loss: 0.1815\n",
      "Epoch 159/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3181 - val_loss: 0.1800\n",
      "Epoch 160/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3678 - val_loss: 0.1770\n",
      "Epoch 161/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3816 - val_loss: 0.1734\n",
      "Epoch 162/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2643 - val_loss: 0.1702\n",
      "Epoch 163/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3316 - val_loss: 0.1678\n",
      "Epoch 164/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3470 - val_loss: 0.1653\n",
      "Epoch 165/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2901 - val_loss: 0.1617\n",
      "Epoch 166/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3140 - val_loss: 0.1591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3288 - val_loss: 0.1552\n",
      "Epoch 168/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2979 - val_loss: 0.1537\n",
      "Epoch 169/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2263 - val_loss: 0.1519\n",
      "Epoch 170/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3535 - val_loss: 0.1522\n",
      "Epoch 171/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3546 - val_loss: 0.1507\n",
      "Epoch 172/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2664 - val_loss: 0.1495\n",
      "Epoch 173/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3173 - val_loss: 0.1493\n",
      "Epoch 174/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3462 - val_loss: 0.1503\n",
      "Epoch 175/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2357 - val_loss: 0.1506\n",
      "Epoch 176/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2573 - val_loss: 0.1495\n",
      "Epoch 177/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2947 - val_loss: 0.1500\n",
      "Epoch 178/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2355 - val_loss: 0.1485\n",
      "Epoch 179/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2589 - val_loss: 0.1481\n",
      "Epoch 180/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2566 - val_loss: 0.1454\n",
      "Epoch 181/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2884 - val_loss: 0.1424\n",
      "Epoch 182/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3487 - val_loss: 0.1415\n",
      "Epoch 183/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2470 - val_loss: 0.1417\n",
      "Epoch 184/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2986 - val_loss: 0.1396\n",
      "Epoch 185/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2783 - val_loss: 0.1360\n",
      "Epoch 186/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2224 - val_loss: 0.1337\n",
      "Epoch 187/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2376 - val_loss: 0.1336\n",
      "Epoch 188/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2425 - val_loss: 0.1320\n",
      "Epoch 189/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2800 - val_loss: 0.1314\n",
      "Epoch 190/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3123 - val_loss: 0.1321\n",
      "Epoch 191/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2268 - val_loss: 0.1317\n",
      "Epoch 192/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3060 - val_loss: 0.1313\n",
      "Epoch 193/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2731 - val_loss: 0.1322\n",
      "Epoch 194/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2934 - val_loss: 0.1301\n",
      "Epoch 195/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2563 - val_loss: 0.1279\n",
      "Epoch 196/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1993 - val_loss: 0.1280\n",
      "Epoch 197/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2059 - val_loss: 0.1267\n",
      "Epoch 198/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1879 - val_loss: 0.1258\n",
      "Epoch 199/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2636 - val_loss: 0.1262\n",
      "Epoch 200/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2644 - val_loss: 0.1260\n",
      "Epoch 201/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2779 - val_loss: 0.1269\n",
      "Epoch 202/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2980 - val_loss: 0.1273\n",
      "Epoch 203/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2650 - val_loss: 0.1262\n",
      "Epoch 204/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2567 - val_loss: 0.1254\n",
      "Epoch 205/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2876 - val_loss: 0.1250\n",
      "Epoch 206/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2726 - val_loss: 0.1261\n",
      "Epoch 207/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1881 - val_loss: 0.1261\n",
      "Epoch 208/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2577 - val_loss: 0.1252\n",
      "Epoch 209/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2191 - val_loss: 0.1239\n",
      "Epoch 210/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1895 - val_loss: 0.1245\n",
      "Epoch 211/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2389 - val_loss: 0.1241\n",
      "Epoch 212/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2181 - val_loss: 0.1220\n",
      "Epoch 213/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2611 - val_loss: 0.1189\n",
      "Epoch 214/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2826 - val_loss: 0.1181\n",
      "Epoch 215/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1953 - val_loss: 0.1181\n",
      "Epoch 216/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1870 - val_loss: 0.1179\n",
      "Epoch 217/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2418 - val_loss: 0.1188\n",
      "Epoch 218/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2860 - val_loss: 0.1196\n",
      "Epoch 219/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1941 - val_loss: 0.1195\n",
      "Epoch 220/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2512 - val_loss: 0.1187\n",
      "Epoch 221/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1908 - val_loss: 0.1193\n",
      "Epoch 222/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2178 - val_loss: 0.1213\n",
      "Epoch 223/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2369 - val_loss: 0.1233\n",
      "Epoch 224/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2562 - val_loss: 0.1230\n",
      "Epoch 225/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1927 - val_loss: 0.1220\n",
      "Epoch 226/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2669 - val_loss: 0.1209\n",
      "Epoch 227/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1997 - val_loss: 0.1216\n",
      "Epoch 228/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1950 - val_loss: 0.1232\n",
      "Epoch 229/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2539 - val_loss: 0.1219\n",
      "Epoch 230/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1629 - val_loss: 0.1201\n",
      "Epoch 231/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1968 - val_loss: 0.1211\n",
      "Epoch 232/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1861 - val_loss: 0.1208\n",
      "Epoch 233/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1665 - val_loss: 0.1221\n",
      "Epoch 234/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1940 - val_loss: 0.1205\n",
      "Epoch 235/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2339 - val_loss: 0.1179\n",
      "Epoch 236/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1916 - val_loss: 0.1176\n",
      "Epoch 237/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2012 - val_loss: 0.1177\n",
      "Epoch 238/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1748 - val_loss: 0.1202\n",
      "Epoch 239/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2076 - val_loss: 0.1228\n",
      "Epoch 240/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2555 - val_loss: 0.1239\n",
      "Epoch 241/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2042 - val_loss: 0.1235\n",
      "Epoch 242/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2432 - val_loss: 0.1216\n",
      "Epoch 243/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2462 - val_loss: 0.1203\n",
      "Epoch 244/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1476 - val_loss: 0.1190\n",
      "Epoch 245/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2256 - val_loss: 0.1192\n",
      "Epoch 246/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1854 - val_loss: 0.1208\n",
      "Epoch 247/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1660 - val_loss: 0.1203\n",
      "Epoch 248/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1881 - val_loss: 0.1217\n",
      "Epoch 249/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1541 - val_loss: 0.1245\n",
      "Epoch 250/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2505 - val_loss: 0.1230\n",
      "Epoch 251/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1751 - val_loss: 0.1219\n",
      "Epoch 252/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2291 - val_loss: 0.1194\n",
      "Epoch 253/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1557 - val_loss: 0.1204\n",
      "Epoch 254/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2268 - val_loss: 0.1202\n",
      "Epoch 255/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1548 - val_loss: 0.1178\n",
      "Epoch 256/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2250 - val_loss: 0.1163\n",
      "Epoch 257/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2008 - val_loss: 0.1149\n",
      "Epoch 258/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2081 - val_loss: 0.1152\n",
      "Epoch 259/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1617 - val_loss: 0.1164\n",
      "Epoch 260/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2244 - val_loss: 0.1181\n",
      "Epoch 261/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1654 - val_loss: 0.1176\n",
      "Epoch 262/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1564 - val_loss: 0.1190\n",
      "Epoch 263/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1767 - val_loss: 0.1179\n",
      "Epoch 264/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2306 - val_loss: 0.1162\n",
      "Epoch 265/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2035 - val_loss: 0.1150\n",
      "Epoch 266/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2026 - val_loss: 0.1148\n",
      "Epoch 267/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2103 - val_loss: 0.1136\n",
      "Epoch 268/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1752 - val_loss: 0.1118\n",
      "Epoch 269/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1605 - val_loss: 0.1109\n",
      "Epoch 270/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1553 - val_loss: 0.1104\n",
      "Epoch 271/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1836 - val_loss: 0.1117\n",
      "Epoch 272/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2212 - val_loss: 0.1120\n",
      "Epoch 273/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1785 - val_loss: 0.1109\n",
      "Epoch 274/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1560 - val_loss: 0.1099\n",
      "Epoch 275/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1170 - val_loss: 0.1104\n",
      "Epoch 276/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1882 - val_loss: 0.1112\n",
      "Epoch 277/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2101 - val_loss: 0.1116\n",
      "Epoch 278/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1909 - val_loss: 0.1127\n",
      "Epoch 279/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1543 - val_loss: 0.1148\n",
      "Epoch 280/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2072 - val_loss: 0.1160\n",
      "Epoch 281/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2225 - val_loss: 0.1156\n",
      "Epoch 282/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1791 - val_loss: 0.1135\n",
      "Epoch 283/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1513 - val_loss: 0.1097\n",
      "Epoch 284/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1656 - val_loss: 0.1077\n",
      "Epoch 285/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1611 - val_loss: 0.1066\n",
      "Epoch 286/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1545 - val_loss: 0.1047\n",
      "Epoch 287/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2035 - val_loss: 0.1047\n",
      "Epoch 288/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2107 - val_loss: 0.1063\n",
      "Epoch 289/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1778 - val_loss: 0.1069\n",
      "Epoch 290/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1455 - val_loss: 0.1070\n",
      "Epoch 291/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1564 - val_loss: 0.1071\n",
      "Epoch 292/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1846 - val_loss: 0.1063\n",
      "Epoch 293/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1555 - val_loss: 0.1057\n",
      "Epoch 294/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1585 - val_loss: 0.1056\n",
      "Epoch 295/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1539 - val_loss: 0.1041\n",
      "Epoch 296/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1594 - val_loss: 0.1035\n",
      "Epoch 297/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1833 - val_loss: 0.1032\n",
      "Epoch 298/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1253 - val_loss: 0.1053\n",
      "Epoch 299/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1934 - val_loss: 0.1081\n",
      "Epoch 300/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2121 - val_loss: 0.1094\n",
      "Epoch 301/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1226 - val_loss: 0.1125\n",
      "Epoch 302/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1776 - val_loss: 0.1151\n",
      "Epoch 303/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1418 - val_loss: 0.1156\n",
      "Epoch 304/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1176 - val_loss: 0.1134\n",
      "Epoch 305/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1569 - val_loss: 0.1134\n",
      "Epoch 306/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1754 - val_loss: 0.1142\n",
      "Epoch 307/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1830 - val_loss: 0.1131\n",
      "Epoch 308/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1432 - val_loss: 0.1099\n",
      "Epoch 309/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1634 - val_loss: 0.1101\n",
      "Epoch 310/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1162 - val_loss: 0.1105\n",
      "Epoch 311/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1293 - val_loss: 0.1102\n",
      "Epoch 312/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2011 - val_loss: 0.1074\n",
      "Epoch 313/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1211 - val_loss: 0.1049\n",
      "Epoch 314/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1503 - val_loss: 0.1037\n",
      "Epoch 315/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1795 - val_loss: 0.1014\n",
      "Epoch 316/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1747 - val_loss: 0.1005\n",
      "Epoch 317/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1329 - val_loss: 0.1003\n",
      "Epoch 318/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1925 - val_loss: 0.1007\n",
      "Epoch 319/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1288 - val_loss: 0.1020\n",
      "Epoch 320/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1578 - val_loss: 0.1034\n",
      "Epoch 321/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1605 - val_loss: 0.1065\n",
      "Epoch 322/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1175 - val_loss: 0.1063\n",
      "Epoch 323/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1125 - val_loss: 0.1058\n",
      "Epoch 324/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1422 - val_loss: 0.1053\n",
      "Epoch 325/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1553 - val_loss: 0.1050\n",
      "Epoch 326/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1556 - val_loss: 0.1046\n",
      "Epoch 327/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1086 - val_loss: 0.1058\n",
      "Epoch 328/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1611 - val_loss: 0.1055\n",
      "Epoch 329/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1394 - val_loss: 0.1067\n",
      "Epoch 330/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1190 - val_loss: 0.1086\n",
      "Epoch 331/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1292 - val_loss: 0.1094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 332/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1097 - val_loss: 0.1088\n",
      "Epoch 333/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1272 - val_loss: 0.1074\n",
      "Epoch 334/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1521 - val_loss: 0.1050\n",
      "Epoch 335/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1371 - val_loss: 0.1032\n",
      "Epoch 336/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1347 - val_loss: 0.1017\n",
      "Epoch 337/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1556 - val_loss: 0.1004\n",
      "Epoch 338/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1694 - val_loss: 0.0997\n",
      "Epoch 339/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1467 - val_loss: 0.0973\n",
      "Epoch 340/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1008 - val_loss: 0.0964\n",
      "Epoch 341/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1099 - val_loss: 0.0971\n",
      "Epoch 342/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1215 - val_loss: 0.0977\n",
      "Epoch 343/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1223 - val_loss: 0.0974\n",
      "Epoch 344/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1442 - val_loss: 0.0989\n",
      "Epoch 345/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1307 - val_loss: 0.1008\n",
      "Epoch 346/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1730 - val_loss: 0.1023\n",
      "Epoch 347/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1278 - val_loss: 0.1020\n",
      "Epoch 348/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1377 - val_loss: 0.1004\n",
      "Epoch 349/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1076 - val_loss: 0.0991\n",
      "Epoch 350/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1476 - val_loss: 0.0975\n",
      "Epoch 351/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1646 - val_loss: 0.0961\n",
      "Epoch 352/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1211 - val_loss: 0.0948\n",
      "Epoch 353/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1283 - val_loss: 0.0949\n",
      "Epoch 354/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1553 - val_loss: 0.0943\n",
      "Epoch 355/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1831 - val_loss: 0.0940\n",
      "Epoch 356/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1182 - val_loss: 0.0946\n",
      "Epoch 357/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1284 - val_loss: 0.0934\n",
      "Epoch 358/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1266 - val_loss: 0.0943\n",
      "Epoch 359/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1550 - val_loss: 0.0954\n",
      "Epoch 360/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1407 - val_loss: 0.0970\n",
      "Epoch 361/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0895 - val_loss: 0.0992\n",
      "Epoch 362/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1359 - val_loss: 0.1017\n",
      "Epoch 363/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1421 - val_loss: 0.1029\n",
      "Epoch 364/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1568 - val_loss: 0.1037\n",
      "Epoch 365/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1404 - val_loss: 0.1017\n",
      "Epoch 366/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1726 - val_loss: 0.0986\n",
      "Epoch 367/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1422 - val_loss: 0.0958\n",
      "Epoch 368/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1383 - val_loss: 0.0939\n",
      "Epoch 369/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2007 - val_loss: 0.0957\n",
      "Epoch 370/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1300 - val_loss: 0.0968\n",
      "Epoch 371/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1292 - val_loss: 0.0965\n",
      "Epoch 372/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1298 - val_loss: 0.0956\n",
      "Epoch 373/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1191 - val_loss: 0.0960\n",
      "Epoch 374/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1455 - val_loss: 0.0980\n",
      "Epoch 375/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1049 - val_loss: 0.1001\n",
      "Epoch 376/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1340 - val_loss: 0.1016\n",
      "Epoch 377/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1147 - val_loss: 0.1027\n",
      "Epoch 378/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1281 - val_loss: 0.1032\n",
      "Epoch 379/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1359 - val_loss: 0.1017\n",
      "Epoch 380/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1133 - val_loss: 0.1011\n",
      "Epoch 381/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1576 - val_loss: 0.1008\n",
      "Epoch 382/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1508 - val_loss: 0.0995\n",
      "Epoch 383/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1216 - val_loss: 0.0989\n",
      "Epoch 384/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1059 - val_loss: 0.0976\n",
      "Epoch 385/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1231 - val_loss: 0.0960\n",
      "Epoch 386/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1264 - val_loss: 0.0946\n",
      "Epoch 387/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1153 - val_loss: 0.0950\n",
      "Epoch 388/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1855 - val_loss: 0.0956\n",
      "Epoch 389/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.1098 - val_loss: 0.0976\n",
      "Epoch 390/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1628 - val_loss: 0.0989\n",
      "Epoch 391/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1060 - val_loss: 0.0991\n",
      "Epoch 392/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1139 - val_loss: 0.0991\n",
      "Epoch 393/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1350 - val_loss: 0.0982\n",
      "Epoch 394/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1400 - val_loss: 0.0966\n",
      "Epoch 395/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1222 - val_loss: 0.0973\n",
      "Epoch 396/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1168 - val_loss: 0.0975\n",
      "Epoch 397/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1207 - val_loss: 0.0978\n",
      "Epoch 398/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1233 - val_loss: 0.0976\n",
      "Epoch 399/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1102 - val_loss: 0.0971\n",
      "Epoch 400/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1512 - val_loss: 0.0953\n",
      "Epoch 401/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1601 - val_loss: 0.0950\n",
      "Epoch 402/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1556 - val_loss: 0.0965\n",
      "Epoch 403/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1233 - val_loss: 0.0983\n",
      "Epoch 404/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1173 - val_loss: 0.0989\n",
      "Epoch 405/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1291 - val_loss: 0.0984\n",
      "Epoch 406/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1387 - val_loss: 0.0971\n",
      "Epoch 407/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1047 - val_loss: 0.0979\n",
      "Epoch 408/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1138 - val_loss: 0.0980\n",
      "Epoch 409/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1109 - val_loss: 0.0981\n",
      "Epoch 410/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1106 - val_loss: 0.0974\n",
      "Epoch 411/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1393 - val_loss: 0.0972\n",
      "Epoch 412/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1152 - val_loss: 0.0970\n",
      "Epoch 413/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1867 - val_loss: 0.0980\n",
      "Epoch 414/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1014 - val_loss: 0.0979\n",
      "Epoch 415/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1470 - val_loss: 0.0971\n",
      "Epoch 416/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1068 - val_loss: 0.0955\n",
      "Epoch 417/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1375 - val_loss: 0.0927\n",
      "Epoch 418/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1147 - val_loss: 0.0920\n",
      "Epoch 419/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0991 - val_loss: 0.0901\n",
      "Epoch 420/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1456 - val_loss: 0.0882\n",
      "Epoch 421/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1096 - val_loss: 0.0862\n",
      "Epoch 422/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0967 - val_loss: 0.0863\n",
      "Epoch 423/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1046 - val_loss: 0.0864\n",
      "Epoch 424/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1053 - val_loss: 0.0865\n",
      "Epoch 425/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1174 - val_loss: 0.0875\n",
      "Epoch 426/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1391 - val_loss: 0.0877\n",
      "Epoch 427/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1160 - val_loss: 0.0881\n",
      "Epoch 428/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1409 - val_loss: 0.0882\n",
      "Epoch 429/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1195 - val_loss: 0.0874\n",
      "Epoch 430/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1663 - val_loss: 0.0873\n",
      "Epoch 431/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1074 - val_loss: 0.0866\n",
      "Epoch 432/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1241 - val_loss: 0.0867\n",
      "Epoch 433/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1008 - val_loss: 0.0869\n",
      "Epoch 434/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0994 - val_loss: 0.0866\n",
      "Epoch 435/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1081 - val_loss: 0.0868\n",
      "Epoch 436/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1015 - val_loss: 0.0883\n",
      "Epoch 437/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1227 - val_loss: 0.0908\n",
      "Epoch 438/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1423 - val_loss: 0.0937\n",
      "Epoch 439/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1597 - val_loss: 0.0944\n",
      "Epoch 440/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0768 - val_loss: 0.0946\n",
      "Epoch 441/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1173 - val_loss: 0.0942\n",
      "Epoch 442/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1228 - val_loss: 0.0938\n",
      "Epoch 443/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1558 - val_loss: 0.0924\n",
      "Epoch 444/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1265 - val_loss: 0.0918\n",
      "Epoch 445/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1149 - val_loss: 0.0932\n",
      "Epoch 446/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0899 - val_loss: 0.0929\n",
      "Epoch 447/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1001 - val_loss: 0.0935\n",
      "Epoch 448/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.0875 - val_loss: 0.0938\n",
      "Epoch 449/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1620 - val_loss: 0.0944\n",
      "Epoch 450/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1000 - val_loss: 0.0945\n",
      "Epoch 451/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0948 - val_loss: 0.0918\n",
      "Epoch 452/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1061 - val_loss: 0.0906\n",
      "Epoch 453/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1381 - val_loss: 0.0900\n",
      "Epoch 454/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.0884 - val_loss: 0.0892\n",
      "Epoch 455/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1248 - val_loss: 0.0900\n",
      "Epoch 456/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1167 - val_loss: 0.0900\n",
      "Epoch 457/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.0951 - val_loss: 0.0895\n",
      "Epoch 458/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1432 - val_loss: 0.0886\n",
      "Epoch 459/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1043 - val_loss: 0.0900\n",
      "Epoch 460/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1136 - val_loss: 0.0916\n",
      "Epoch 461/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.0892 - val_loss: 0.0913\n",
      "Epoch 462/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0755 - val_loss: 0.0892\n",
      "Epoch 463/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1029 - val_loss: 0.0881\n",
      "Epoch 464/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.0901 - val_loss: 0.0869\n",
      "Epoch 465/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1112 - val_loss: 0.0857\n",
      "Epoch 466/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1208 - val_loss: 0.0848\n",
      "Epoch 467/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1177 - val_loss: 0.0846\n",
      "Epoch 468/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1134 - val_loss: 0.0845\n",
      "Epoch 469/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.0864 - val_loss: 0.0852\n",
      "Epoch 470/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1114 - val_loss: 0.0853\n",
      "Epoch 471/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1488 - val_loss: 0.0855\n",
      "Epoch 472/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1156 - val_loss: 0.0857\n",
      "Epoch 473/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1234 - val_loss: 0.0865\n",
      "Epoch 474/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1080 - val_loss: 0.0878\n",
      "Epoch 475/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0959 - val_loss: 0.0886\n",
      "Epoch 476/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1484 - val_loss: 0.0892\n",
      "Epoch 477/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1456 - val_loss: 0.0879\n",
      "Epoch 478/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1101 - val_loss: 0.0858\n",
      "Epoch 479/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.0928 - val_loss: 0.0845\n",
      "Epoch 480/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1103 - val_loss: 0.0845\n",
      "Epoch 481/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1109 - val_loss: 0.0843\n",
      "Epoch 482/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1455 - val_loss: 0.0848\n",
      "Epoch 483/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.0947 - val_loss: 0.0865\n",
      "Epoch 484/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1154 - val_loss: 0.0877\n",
      "Epoch 485/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.0877 - val_loss: 0.0883\n",
      "Epoch 486/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0895 - val_loss: 0.0901\n",
      "Epoch 487/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.0946 - val_loss: 0.0911\n",
      "Epoch 488/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1026 - val_loss: 0.0925\n",
      "Epoch 489/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0858 - val_loss: 0.0931\n",
      "Epoch 490/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1091 - val_loss: 0.0898\n",
      "Epoch 491/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1017 - val_loss: 0.0869\n",
      "Epoch 492/500\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1143 - val_loss: 0.0843\n",
      "Epoch 493/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1151 - val_loss: 0.0837\n",
      "Epoch 494/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1450 - val_loss: 0.0838\n",
      "Epoch 495/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0838 - val_loss: 0.0818\n",
      "Epoch 496/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1167 - val_loss: 0.0804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 497/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1120 - val_loss: 0.0794\n",
      "Epoch 498/500\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0992 - val_loss: 0.0785\n",
      "Epoch 499/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1220 - val_loss: 0.0791\n",
      "Epoch 500/500\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1158 - val_loss: 0.0783\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit(\n",
    "    train_x, train_y,\n",
    "    epochs=500,verbose=1,\n",
    "    # Calculate validation results on 10% of the training data\n",
    "    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y=model1.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ansQ1=df[280:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-155-5d62f438ef1e>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ansQ1['y1']=test_y[:,0]\n",
      "<ipython-input-155-5d62f438ef1e>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ansQ1['y2']=test_y[:,1]\n"
     ]
    }
   ],
   "source": [
    "df_ansQ1['y1']=test_y[:,0]\n",
    "df_ansQ1['y2']=test_y[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No.</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>281</td>\n",
       "      <td>10.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.639224</td>\n",
       "      <td>-0.459265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>282</td>\n",
       "      <td>5.4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>12.6</td>\n",
       "      <td>-0.704865</td>\n",
       "      <td>0.801001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>283</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-12.1</td>\n",
       "      <td>21.4</td>\n",
       "      <td>-9.1</td>\n",
       "      <td>1.075400</td>\n",
       "      <td>-1.107367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>284</td>\n",
       "      <td>-10.2</td>\n",
       "      <td>11.5</td>\n",
       "      <td>-4.3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.388047</td>\n",
       "      <td>0.247255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>285</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>-2.7</td>\n",
       "      <td>0.495829</td>\n",
       "      <td>-0.550414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>286</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-14.1</td>\n",
       "      <td>-19.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.965128</td>\n",
       "      <td>0.854465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>287</td>\n",
       "      <td>-10.3</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>-5.2</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.472612</td>\n",
       "      <td>-0.609686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>288</td>\n",
       "      <td>2.4</td>\n",
       "      <td>5.7</td>\n",
       "      <td>8.1</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>0.652088</td>\n",
       "      <td>-0.643321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>289</td>\n",
       "      <td>-25.9</td>\n",
       "      <td>-10.5</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>-6.4</td>\n",
       "      <td>0.362646</td>\n",
       "      <td>-0.848616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>290</td>\n",
       "      <td>-2.9</td>\n",
       "      <td>12.2</td>\n",
       "      <td>11.1</td>\n",
       "      <td>9.6</td>\n",
       "      <td>-0.416014</td>\n",
       "      <td>0.483523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>291</td>\n",
       "      <td>11.7</td>\n",
       "      <td>13.3</td>\n",
       "      <td>-8.1</td>\n",
       "      <td>9.6</td>\n",
       "      <td>-0.816292</td>\n",
       "      <td>0.949931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>292</td>\n",
       "      <td>17.2</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>8.6</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.915336</td>\n",
       "      <td>-0.645613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>293</td>\n",
       "      <td>-8.4</td>\n",
       "      <td>-15.3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>-4.2</td>\n",
       "      <td>-0.881904</td>\n",
       "      <td>0.615833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>294</td>\n",
       "      <td>-12.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.7</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.786734</td>\n",
       "      <td>-0.939100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>295</td>\n",
       "      <td>1.3</td>\n",
       "      <td>-14.1</td>\n",
       "      <td>-4.3</td>\n",
       "      <td>7.8</td>\n",
       "      <td>-1.617966</td>\n",
       "      <td>1.649421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>296</td>\n",
       "      <td>-13.3</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>11.7</td>\n",
       "      <td>-5.7</td>\n",
       "      <td>0.917265</td>\n",
       "      <td>-1.110370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>297</td>\n",
       "      <td>1.2</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-16.6</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>0.287676</td>\n",
       "      <td>-0.373234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>298</td>\n",
       "      <td>-8.1</td>\n",
       "      <td>4.6</td>\n",
       "      <td>7.9</td>\n",
       "      <td>12.9</td>\n",
       "      <td>0.550810</td>\n",
       "      <td>-0.566360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>299</td>\n",
       "      <td>-7.3</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>11.8</td>\n",
       "      <td>0.541752</td>\n",
       "      <td>-0.573728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>300</td>\n",
       "      <td>10.3</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.6</td>\n",
       "      <td>-1.300323</td>\n",
       "      <td>1.497015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     No.    x1    x2    x3    x4        y1        y2\n",
       "280  281  10.8   0.7   0.7   4.8  0.639224 -0.459265\n",
       "281  282   5.4  12.0  -1.6  12.6 -0.704865  0.801001\n",
       "282  283  -5.0 -12.1  21.4  -9.1  1.075400 -1.107367\n",
       "283  284 -10.2  11.5  -4.3   0.8 -0.388047  0.247255\n",
       "284  285  -0.9   2.1  -4.8  -2.7  0.495829 -0.550414\n",
       "285  286  -1.0 -14.1 -19.2   0.5 -0.965128  0.854465\n",
       "286  287 -10.3  -1.1  -5.2   6.8  0.472612 -0.609686\n",
       "287  288   2.4   5.7   8.1  -6.0  0.652088 -0.643321\n",
       "288  289 -25.9 -10.5  -1.5  -6.4  0.362646 -0.848616\n",
       "289  290  -2.9  12.2  11.1   9.6 -0.416014  0.483523\n",
       "290  291  11.7  13.3  -8.1   9.6 -0.816292  0.949931\n",
       "291  292  17.2  -1.1   8.6  -0.8  0.915336 -0.645613\n",
       "292  293  -8.4 -15.3   4.6  -4.2 -0.881904  0.615833\n",
       "293  294 -12.1   2.0  11.7  -0.3  0.786734 -0.939100\n",
       "294  295   1.3 -14.1  -4.3   7.8 -1.617966  1.649421\n",
       "295  296 -13.3  -1.7  11.7  -5.7  0.917265 -1.110370\n",
       "296  297   1.2  -7.0 -16.6  -1.2  0.287676 -0.373234\n",
       "297  298  -8.1   4.6   7.9  12.9  0.550810 -0.566360\n",
       "298  299  -7.3  -1.0   0.8  11.8  0.541752 -0.573728\n",
       "299  300  10.3 -16.0   3.2   2.6 -1.300323  1.497015"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ansQ1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "#### 1.Any prepocessing?  \n",
    "我在模型加入Normalization層，對每個欄位對標準化\n",
    "#### 2.How you get the best model ? \n",
    "我反覆嘗試，並跟據validation 的loss 來找到最好的模型架構以及參數\n",
    "#### 3.State all the parameters you need for training  (learning rates, epochs, weight decay, moment, etc.) \n",
    "learning rates為0.00003， epochs為500，然後我不是使用weight decay，而是使用drop out層來防止 overfitting\n",
    "#### 4.Show the structure of your best model\n",
    "架構如上面 model1.summary( ) 所顯示\n",
    "#### 5.Write down the prediction of testing data (y1, y2)  (list in a table such as Table 1)\n",
    "如上表 df_ansQ1 顯示\n",
    "#### 6.Plot the learning curve (MSE) \n",
    "如上圖 plot_loss(history) 顯示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
